{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c52cd2",
   "metadata": {},
   "source": [
    "陈景韬 523030910028\n",
    "\n",
    "(1)文本对抗攻击:\n",
    "\n",
    "(a) 利用框架定义好的攻击方法实现单个文本攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4ed1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\陈景韬\\AppData\\Roaming\\nltk_data...\n",
      "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (99%) --> 0 (100%)\n",
      "\n",
      "I really enjoyed the new movie that came out last month\n",
      "\n",
      "I really savour the unexampled movie that came out last month\n"
     ]
    }
   ],
   "source": [
    "# Use PWWS attack from TextAttack library\n",
    "\n",
    "import textattack\n",
    "import transformers\n",
    "from textattack.attack_recipes import PWWSRen2019\n",
    "\n",
    "# load model, tokenizer and model_wrapper\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "attack = PWWSRen2019.build(model_wrapper)\n",
    "\n",
    "input_text = \"I really enjoyed the new movie that came out last month\"\n",
    "label = 1\n",
    "attack_result = attack.attack(input_text, label)\n",
    "print(attack_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea57cbf",
   "metadata": {},
   "source": [
    "(b)利用四大组件组装攻击方法实现单个文本攻击(要求与所选择的攻击匹配)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a48efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\陈景韬\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (99%) --> 0 (74%)\n",
      "\n",
      "I really enjoyed the new movie that came out last month\n",
      "\n",
      "I really love the unexampled flick that came out last-place month\n"
     ]
    }
   ],
   "source": [
    "import textattack\n",
    "import transformers\n",
    "from textattack.transformations import WordSwapWordNet\n",
    "from textattack.constraints.pre_transformation import(\n",
    "    StopwordModification,\n",
    "    RepeatModification,\n",
    ")\n",
    "from textattack.constraints.grammaticality import PartOfSpeech\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.search_methods import GreedyWordSwapWIR\n",
    "\n",
    "# load model, tokenizer and model_wrapper\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# 变换方法\n",
    "transformation = WordSwapWordNet(language=\"eng\")\n",
    "\n",
    "# 约束条件\n",
    "stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"you\", \"your\", \"yours\",\n",
    "    \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"it\", \"its\", \"they\", \"them\",\n",
    "    \"their\", \"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"to\", \"is\", \"of\", \"and\", \"or\"\n",
    "])\n",
    "constraints = [\n",
    "    RepeatModification(),\n",
    "    StopwordModification(stopwords=stopwords),\n",
    "    PartOfSpeech(),  # 保证词性一致\n",
    "    WordEmbeddingDistance(min_cos_sim=0.5)  # 保证语义距离不太大\n",
    "]\n",
    "\n",
    "# 目标函数\n",
    "goal_function = UntargetedClassification(model_wrapper)\n",
    "\n",
    "# 搜索方法\n",
    "search_method = GreedyWordSwapWIR(wir_method='unk') # 'unk'或'delete'都可\n",
    "\n",
    "attack = textattack.Attack(goal_function, constraints, transformation, search_method)\n",
    "\n",
    "input_text = \"I really enjoyed the new movie that came out last month\"\n",
    "label = 1\n",
    "attack_result = attack.attack(input_text, label)\n",
    "print(attack_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c673c",
   "metadata": {},
   "source": [
    "(c)利用四大组件实现对数据集的对抗攻击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b93621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\陈景韬\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94mglue\u001b[0m, subset \u001b[94msst2\u001b[0m, split \u001b[94mvalidation\u001b[0m.\n",
      "textattack: Logging to CSV at path log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  unk\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapWordNet\n",
      "  (constraints): \n",
      "    (0): PartOfSpeech(\n",
      "        (tagger_type):  nltk\n",
      "        (tagset):  universal\n",
      "        (allow_verb_noun_swap):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): RepeatModification\n",
      "    (3): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 1 / 2 / 5:  25%|██▌       | 5/20 [00:19<00:58,  3.87s/it]textattack: Saving checkpoint under \"checkpoints\\1764502889352.ta.chkpt\" at 2025-11-30 19:41:29 after 5 attacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 5 / 2 / 10:  50%|█████     | 10/20 [01:11<01:11,  7.12s/it]textattack: Saving checkpoint under \"checkpoints\\1764502941211.ta.chkpt\" at 2025-11-30 19:42:21 after 10 attacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 6 / 2 / 15:  75%|███████▌  | 15/20 [01:53<00:37,  7.59s/it]textattack: Saving checkpoint under \"checkpoints\\1764502983769.ta.chkpt\" at 2025-11-30 19:43:03 after 15 attacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 11 / 7 / 2 / 20: 100%|██████████| 20/20 [02:27<00:00,  7.36s/it]textattack: Saving checkpoint under \"checkpoints\\1764503017259.ta.chkpt\" at 2025-11-30 19:43:37 after 20 attacks.\n",
      "[Succeeded / Failed / Skipped / Total] 11 / 7 / 2 / 20: 100%|██████████| 20/20 [02:27<00:00,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=============================================================================================================================\n",
      "=============================================================================================================================\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 11     |\n",
      "| Number of failed attacks:     | 7      |\n",
      "| Number of skipped attacks:    | 2      |\n",
      "| Original accuracy:            | 90.0%  |\n",
      "| Accuracy under attack:        | 35.0%  |\n",
      "| Attack success rate:          | 61.11% |\n",
      "| Average perturbed word %:     | 16.99% |\n",
      "| Average num. words per input: | 15.8   |\n",
      "| Avg num queries:              | 31.44  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<textattack.attack_results.failed_attack_result.FailedAttackResult at 0x233cc8a80b0>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x233d06c4cb0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233c30651f0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233c50db1a0>,\n",
       " <textattack.attack_results.skipped_attack_result.SkippedAttackResult at 0x233d06c6e10>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x23326e07aa0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233d06c6150>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x233d06c4da0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x233c59eec30>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x233e1d9d640>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x233fa52bad0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233fad4d940>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233f6d9dca0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233ec47bcb0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233c59c73b0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x233c59eef30>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233faba1490>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233f6d9ede0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233f685d910>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x233f6af0f50>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textattack\n",
    "import transformers\n",
    "from textattack.transformations import WordSwapWordNet\n",
    "from textattack.constraints.pre_transformation import(\n",
    "    StopwordModification,\n",
    "    RepeatModification,\n",
    ")\n",
    "from textattack.constraints.grammaticality import PartOfSpeech\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.search_methods import GreedyWordSwapWIR\n",
    "\n",
    "# load model, tokenizer and model_wrapper\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
    "model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# 变换方法\n",
    "transformation = WordSwapWordNet(language=\"eng\")\n",
    "\n",
    "# 约束条件\n",
    "stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"you\", \"your\", \"yours\",\n",
    "    \"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"it\", \"its\", \"they\", \"them\",\n",
    "    \"their\", \"the\", \"a\", \"an\", \"in\", \"on\", \"at\", \"to\", \"is\", \"of\", \"and\", \"or\"\n",
    "])\n",
    "constraints = [\n",
    "    RepeatModification(),\n",
    "    StopwordModification(stopwords=stopwords),\n",
    "    PartOfSpeech(),  # 保证词性一致\n",
    "    WordEmbeddingDistance(min_cos_sim=0.5)  # 保证语义距离不太大\n",
    "]\n",
    "\n",
    "# 目标函数\n",
    "goal_function = UntargetedClassification(model_wrapper)\n",
    "\n",
    "# 搜索方法\n",
    "search_method = GreedyWordSwapWIR(wir_method='unk') # 'unk'或'delete'都可\n",
    "\n",
    "attack = textattack.Attack(goal_function, constraints, transformation, search_method)\n",
    "\n",
    "dataset = textattack.datasets.HuggingFaceDataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples = 20,\n",
    "    log_to_csv=\"log.csv\",\n",
    "    checkpoint_interval=5,\n",
    "    checkpoint_dir=\"checkpoints\",\n",
    "    disable_stdout=True\n",
    ")\n",
    "\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "\n",
    "attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020176f0",
   "metadata": {},
   "source": [
    "（The result stored into ./log.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cefc7e",
   "metadata": {},
   "source": [
    "(2) 文本数据增强:\n",
    "\n",
    "(a)利用框架定义的增强方式实现两个文本的数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22dc0f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting data...: 100%|██████████| 2/2 [00:00<00:00, 63.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['What I cannot creating, I do not understand.'], ['Whereof can I say?']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "\n",
    "augmenter = EmbeddingAugmenter()\n",
    "\n",
    "s = \"What I cannot create, I do not understand.\"\n",
    "s1 = \"What can I say?\"\n",
    "\n",
    "text_list = [s, s1]\n",
    "\n",
    "augmenter.augment_many(text_list, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7412d0",
   "metadata": {},
   "source": [
    "(b)利用自定义的文本增强实现两个文本的数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b6d85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting data...: 100%|██████████| 2/2 [00:00<00:00, 377.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Wat I cannot create, I do not understand.',\n",
       "  'What I cannot create, I d not understand.',\n",
       "  'What I cannot create, I do nt understand.',\n",
       "  'What I cannot create, I o not understand.',\n",
       "  'What I cannot crete, I do not understand.'],\n",
       " ['Wat can I say?',\n",
       "  'What an I say?',\n",
       "  'What ca I say?',\n",
       "  'What cn I say?',\n",
       "  'hat can I say?']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "from textattack.transformations import CompositeTransformation\n",
    "from textattack.augmentation import Augmenter\n",
    "\n",
    "transformation = CompositeTransformation([WordSwapRandomCharacterDeletion()])\n",
    "augmenter = Augmenter(transformation=transformation, transformations_per_example=5)\n",
    "\n",
    "augmenter.augment_many(text_list, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6afc7e",
   "metadata": {},
   "source": [
    "(3)对抗性训练:\n",
    "\n",
    "找到代码并粘贴(不要求运行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a2db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextAttack/textattack/trainer.py\n",
    "\n",
    "\"\"\"\n",
    "Trainer Class\n",
    "=============\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import scipy\n",
    "import torch\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "import textattack\n",
    "from textattack.shared.utils import logger\n",
    "\n",
    "from .attack import Attack\n",
    "from .attack_args import AttackArgs\n",
    "from .attack_results import MaximizedAttackResult, SuccessfulAttackResult\n",
    "from .attacker import Attacker\n",
    "from .model_args import HUGGINGFACE_MODELS\n",
    "from .models.helpers import LSTMForClassification, WordCNNForClassification\n",
    "from .models.wrappers import ModelWrapper\n",
    "from .training_args import CommandLineTrainingArgs, TrainingArgs\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer is training and eval loop for adversarial training.\n",
    "\n",
    "    It is designed to work with PyTorch and Transformers models.\n",
    "\n",
    "    Args:\n",
    "        model_wrapper (:class:`~textattack.models.wrappers.ModelWrapper`):\n",
    "            Model wrapper containing both the model and the tokenizer.\n",
    "        task_type (:obj:`str`, `optional`, defaults to :obj:`\"classification\"`):\n",
    "            The task that the model is trained to perform.\n",
    "            Currently, :class:`~textattack.Trainer` supports two tasks: (1) :obj:`\"classification\"`, (2) :obj:`\"regression\"`.\n",
    "        attack (:class:`~textattack.Attack`):\n",
    "            :class:`~textattack.Attack` used to generate adversarial examples for training.\n",
    "        train_dataset (:class:`~textattack.datasets.Dataset`):\n",
    "            Dataset for training.\n",
    "        eval_dataset (:class:`~textattack.datasets.Dataset`):\n",
    "            Dataset for evaluation\n",
    "        training_args (:class:`~textattack.TrainingArgs`):\n",
    "            Arguments for training.\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> import textattack\n",
    "        >>> import transformers\n",
    "\n",
    "        >>> model = transformers.AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        >>> model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "        >>> # We only use DeepWordBugGao2018 to demonstration purposes.\n",
    "        >>> attack = textattack.attack_recipes.DeepWordBugGao2018.build(model_wrapper)\n",
    "        >>> train_dataset = textattack.datasets.HuggingFaceDataset(\"imdb\", split=\"train\")\n",
    "        >>> eval_dataset = textattack.datasets.HuggingFaceDataset(\"imdb\", split=\"test\")\n",
    "\n",
    "        >>> # Train for 3 epochs with 1 initial clean epochs, 1000 adversarial examples per epoch, learning rate of 5e-5, and effective batch size of 32 (8x4).\n",
    "        >>> training_args = textattack.TrainingArgs(\n",
    "        ...     num_epochs=3,\n",
    "        ...     num_clean_epochs=1,\n",
    "        ...     num_train_adv_examples=1000,\n",
    "        ...     learning_rate=5e-5,\n",
    "        ...     per_device_train_batch_size=8,\n",
    "        ...     gradient_accumulation_steps=4,\n",
    "        ...     log_to_tb=True,\n",
    "        ... )\n",
    "\n",
    "        >>> trainer = textattack.Trainer(\n",
    "        ...     model_wrapper,\n",
    "        ...     \"classification\",\n",
    "        ...     attack,\n",
    "        ...     train_dataset,\n",
    "        ...     eval_dataset,\n",
    "        ...     training_args\n",
    "        ... )\n",
    "        >>> trainer.train()\n",
    "\n",
    "    .. note::\n",
    "        When using :class:`~textattack.Trainer` with `parallel=True` in :class:`~textattack.TrainingArgs`,\n",
    "        make sure to protect the “entry point” of the program by using :obj:`if __name__ == '__main__':`.\n",
    "        If not, each worker process used for generating adversarial examples will execute the training code again.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_wrapper,\n",
    "        task_type=\"classification\",\n",
    "        attack=None,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=None,\n",
    "        training_args=None,\n",
    "    ):\n",
    "        assert isinstance(\n",
    "            model_wrapper, ModelWrapper\n",
    "        ), f\"`model_wrapper` must be of type `textattack.models.wrappers.ModelWrapper`, but got type `{type(model_wrapper)}`.\"\n",
    "\n",
    "        # TODO: Support seq2seq training\n",
    "        assert task_type in {\n",
    "            \"classification\",\n",
    "            \"regression\",\n",
    "        }, '`task_type` must either be \"classification\" or \"regression\"'\n",
    "\n",
    "        if attack:\n",
    "            assert isinstance(\n",
    "                attack, Attack\n",
    "            ), f\"`attack` argument must be of type `textattack.Attack`, but got type of `{type(attack)}`.\"\n",
    "\n",
    "            if id(model_wrapper) != id(attack.goal_function.model):\n",
    "                logger.warn(\n",
    "                    \"`model_wrapper` and the victim model of `attack` are not the same model.\"\n",
    "                )\n",
    "\n",
    "        if train_dataset:\n",
    "            assert isinstance(\n",
    "                train_dataset, textattack.datasets.Dataset\n",
    "            ), f\"`train_dataset` must be of type `textattack.datasets.Dataset`, but got type `{type(train_dataset)}`.\"\n",
    "\n",
    "        if eval_dataset:\n",
    "            assert isinstance(\n",
    "                eval_dataset, textattack.datasets.Dataset\n",
    "            ), f\"`eval_dataset` must be of type `textattack.datasets.Dataset`, but got type `{type(eval_dataset)}`.\"\n",
    "\n",
    "        if training_args:\n",
    "            assert isinstance(\n",
    "                training_args, TrainingArgs\n",
    "            ), f\"`training_args` must be of type `textattack.TrainingArgs`, but got type `{type(training_args)}`.\"\n",
    "        else:\n",
    "            training_args = TrainingArgs()\n",
    "\n",
    "        if not hasattr(model_wrapper, \"model\"):\n",
    "            raise ValueError(\"Cannot detect `model` in `model_wrapper`\")\n",
    "        else:\n",
    "            assert isinstance(\n",
    "                model_wrapper.model, torch.nn.Module\n",
    "            ), f\"`model` in `model_wrapper` must be of type `torch.nn.Module`, but got type `{type(model_wrapper.model)}`.\"\n",
    "\n",
    "        if not hasattr(model_wrapper, \"tokenizer\"):\n",
    "            raise ValueError(\"Cannot detect `tokenizer` in `model_wrapper`\")\n",
    "\n",
    "        self.model_wrapper = model_wrapper\n",
    "        self.task_type = task_type\n",
    "        self.attack = attack\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.training_args = training_args\n",
    "\n",
    "        self._metric_name = (\n",
    "            \"pearson_correlation\" if self.task_type == \"regression\" else \"accuracy\"\n",
    "        )\n",
    "        if self.task_type == \"regression\":\n",
    "            self.loss_fct = torch.nn.MSELoss(reduction=\"none\")\n",
    "        else:\n",
    "            self.loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        self._global_step = 0\n",
    "\n",
    "    def _generate_adversarial_examples(self, epoch):\n",
    "        \"\"\"Generate adversarial examples using attacker.\"\"\"\n",
    "        assert (\n",
    "            self.attack is not None\n",
    "        ), \"`attack` is `None` but attempting to generate adversarial examples.\"\n",
    "        base_file_name = f\"attack-train-{epoch}\"\n",
    "        log_file_name = os.path.join(self.training_args.output_dir, base_file_name)\n",
    "        logger.info(\"Attacking model to generate new adversarial training set...\")\n",
    "\n",
    "        if isinstance(self.training_args.num_train_adv_examples, float):\n",
    "            num_train_adv_examples = math.ceil(\n",
    "                len(self.train_dataset) * self.training_args.num_train_adv_examples\n",
    "            )\n",
    "        else:\n",
    "            num_train_adv_examples = self.training_args.num_train_adv_examples\n",
    "\n",
    "        # Use Different AttackArgs based on num_train_adv_examples value.\n",
    "        # If num_train_adv_examples >= 0 , num_train_adv_examples is\n",
    "        # set as number of successful examples.\n",
    "        # If num_train_adv_examples == -1 , num_examples is set to -1 to\n",
    "        # generate example for all of training data.\n",
    "        if num_train_adv_examples >= 0:\n",
    "            attack_args = AttackArgs(\n",
    "                num_successful_examples=num_train_adv_examples,\n",
    "                num_examples_offset=0,\n",
    "                query_budget=self.training_args.query_budget_train,\n",
    "                shuffle=True,\n",
    "                parallel=self.training_args.parallel,\n",
    "                num_workers_per_device=self.training_args.attack_num_workers_per_device,\n",
    "                disable_stdout=True,\n",
    "                silent=True,\n",
    "                log_to_txt=log_file_name + \".txt\",\n",
    "                log_to_csv=log_file_name + \".csv\",\n",
    "            )\n",
    "        elif num_train_adv_examples == -1:\n",
    "            # set num_examples when num_train_adv_examples = -1\n",
    "            attack_args = AttackArgs(\n",
    "                num_examples=num_train_adv_examples,\n",
    "                num_examples_offset=0,\n",
    "                query_budget=self.training_args.query_budget_train,\n",
    "                shuffle=True,\n",
    "                parallel=self.training_args.parallel,\n",
    "                num_workers_per_device=self.training_args.attack_num_workers_per_device,\n",
    "                disable_stdout=True,\n",
    "                silent=True,\n",
    "                log_to_txt=log_file_name + \".txt\",\n",
    "                log_to_csv=log_file_name + \".csv\",\n",
    "            )\n",
    "        else:\n",
    "            assert False, \"num_train_adv_examples is negative and not equal to -1.\"\n",
    "\n",
    "        attacker = Attacker(self.attack, self.train_dataset, attack_args=attack_args)\n",
    "        results = attacker.attack_dataset()\n",
    "\n",
    "        attack_types = collections.Counter(r.__class__.__name__ for r in results)\n",
    "        total_attacks = (\n",
    "            attack_types[\"SuccessfulAttackResult\"] + attack_types[\"FailedAttackResult\"]\n",
    "        )\n",
    "        success_rate = attack_types[\"SuccessfulAttackResult\"] / total_attacks * 100\n",
    "        logger.info(f\"Total number of attack results: {len(results)}\")\n",
    "        logger.info(\n",
    "            f\"Attack success rate: {success_rate:.2f}% [{attack_types['SuccessfulAttackResult']} / {total_attacks}]\"\n",
    "        )\n",
    "        # TODO: This will produce a bug if we need to manipulate ground truth output.\n",
    "\n",
    "        # To Fix Issue #498 , We need to add the Non Output columns in one tuple to represent input columns\n",
    "        # Since adversarial_example won't be an input to the model , we will have to remove it from the input\n",
    "        # dictionary in collate_fn\n",
    "        adversarial_examples = [\n",
    "            (\n",
    "                tuple(r.perturbed_result.attacked_text._text_input.values())\n",
    "                + (\"adversarial_example\",),\n",
    "                r.perturbed_result.ground_truth_output,\n",
    "            )\n",
    "            for r in results\n",
    "            if isinstance(r, (SuccessfulAttackResult, MaximizedAttackResult))\n",
    "        ]\n",
    "\n",
    "        # Name for column indicating if an example is adversarial is set as \"_example_type\".\n",
    "        adversarial_dataset = textattack.datasets.Dataset(\n",
    "            adversarial_examples,\n",
    "            input_columns=self.train_dataset.input_columns + (\"_example_type\",),\n",
    "            label_map=self.train_dataset.label_map,\n",
    "            label_names=self.train_dataset.label_names,\n",
    "            output_scale_factor=self.train_dataset.output_scale_factor,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return adversarial_dataset\n",
    "\n",
    "    def _print_training_args(\n",
    "        self, total_training_steps, train_batch_size, num_clean_epochs\n",
    "    ):\n",
    "        logger.info(\"***** Running training *****\")\n",
    "        logger.info(f\"  Num examples = {len(self.train_dataset)}\")\n",
    "        logger.info(f\"  Num epochs = {self.training_args.num_epochs}\")\n",
    "        logger.info(f\"  Num clean epochs = {num_clean_epochs}\")\n",
    "        logger.info(\n",
    "            f\"  Instantaneous batch size per device = {self.training_args.per_device_train_batch_size}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"  Total train batch size (w. parallel, distributed & accumulation) = {train_batch_size * self.training_args.gradient_accumulation_steps}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"  Gradient accumulation steps = {self.training_args.gradient_accumulation_steps}\"\n",
    "        )\n",
    "        logger.info(f\"  Total optimization steps = {total_training_steps}\")\n",
    "\n",
    "    def _save_model_checkpoint(\n",
    "        self, model, tokenizer, step=None, epoch=None, best=False, last=False\n",
    "    ):\n",
    "        # Save model checkpoint\n",
    "        if step:\n",
    "            dir_name = f\"checkpoint-step-{step}\"\n",
    "        if epoch:\n",
    "            dir_name = f\"checkpoint-epoch-{epoch}\"\n",
    "        if best:\n",
    "            dir_name = \"best_model\"\n",
    "        if last:\n",
    "            dir_name = \"last_model\"\n",
    "\n",
    "        output_dir = os.path.join(self.training_args.output_dir, dir_name)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            model = model.module\n",
    "\n",
    "        if isinstance(model, (WordCNNForClassification, LSTMForClassification)):\n",
    "            model.save_pretrained(output_dir)\n",
    "        elif isinstance(model, transformers.PreTrainedModel):\n",
    "            model.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "        else:\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            torch.save(\n",
    "                state_dict,\n",
    "                os.path.join(output_dir, \"pytorch_model.bin\"),\n",
    "            )\n",
    "\n",
    "    def _tb_log(self, log, step):\n",
    "        if not hasattr(self, \"_tb_writer\"):\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "            self._tb_writer = SummaryWriter(self.training_args.tb_log_dir)\n",
    "            self._tb_writer.add_hparams(self.training_args.__dict__, {})\n",
    "            self._tb_writer.flush()\n",
    "\n",
    "        for key in log:\n",
    "            self._tb_writer.add_scalar(key, log[key], step)\n",
    "\n",
    "    def _wandb_log(self, log, step):\n",
    "        if not hasattr(self, \"_wandb_init\"):\n",
    "            global wandb\n",
    "            import wandb\n",
    "\n",
    "            self._wandb_init = True\n",
    "            wandb.init(\n",
    "                project=self.training_args.wandb_project,\n",
    "                config=self.training_args.__dict__,\n",
    "            )\n",
    "\n",
    "        wandb.log(log, step=step)\n",
    "\n",
    "    def get_optimizer_and_scheduler(self, model, num_training_steps):\n",
    "        \"\"\"Returns optimizer and scheduler to use for training. If you are\n",
    "        overriding this method and do not want to use a scheduler, simply\n",
    "        return :obj:`None` for scheduler.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`torch.nn.Module`):\n",
    "                Model to be trained. Pass its parameters to optimizer for training.\n",
    "            num_training_steps (:obj:`int`):\n",
    "                Number of total training steps.\n",
    "        Returns:\n",
    "            Tuple of optimizer and scheduler :obj:`tuple[torch.optim.Optimizer, torch.optim.lr_scheduler._LRScheduler]`\n",
    "        \"\"\"\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            model = model.module\n",
    "\n",
    "        if isinstance(model, transformers.PreTrainedModel):\n",
    "            # Reference https://huggingface.co/transformers/training.html\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p\n",
    "                        for n, p in param_optimizer\n",
    "                        if not any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": self.training_args.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            optimizer = transformers.optimization.AdamW(\n",
    "                optimizer_grouped_parameters, lr=self.training_args.learning_rate\n",
    "            )\n",
    "            if isinstance(self.training_args.num_warmup_steps, float):\n",
    "                num_warmup_steps = math.ceil(\n",
    "                    self.training_args.num_warmup_steps * num_training_steps\n",
    "                )\n",
    "            else:\n",
    "                num_warmup_steps = self.training_args.num_warmup_steps\n",
    "\n",
    "            scheduler = transformers.optimization.get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=num_warmup_steps,\n",
    "                num_training_steps=num_training_steps,\n",
    "            )\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(\n",
    "                filter(lambda x: x.requires_grad, model.parameters()),\n",
    "                lr=self.training_args.learning_rate,\n",
    "            )\n",
    "            scheduler = None\n",
    "\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def get_train_dataloader(self, dataset, adv_dataset, batch_size):\n",
    "        \"\"\"Returns the :obj:`torch.utils.data.DataLoader` for training.\n",
    "\n",
    "        Args:\n",
    "            dataset (:class:`~textattack.datasets.Dataset`):\n",
    "                Original training dataset.\n",
    "            adv_dataset (:class:`~textattack.datasets.Dataset`):\n",
    "                Adversarial examples generated from the original training dataset. :obj:`None` if no adversarial attack takes place.\n",
    "            batch_size (:obj:`int`):\n",
    "                Batch size for training.\n",
    "        Returns:\n",
    "            :obj:`torch.utils.data.DataLoader`\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Add pairing option where we can pair original examples with adversarial examples.\n",
    "        # Helper functions for collating data\n",
    "        def collate_fn(data):\n",
    "            input_texts = []\n",
    "            targets = []\n",
    "            is_adv_sample = []\n",
    "            for item in data:\n",
    "                if \"_example_type\" in item[0].keys():\n",
    "                    # Get example type value from OrderedDict and remove it\n",
    "\n",
    "                    adv = item[0].pop(\"_example_type\")\n",
    "\n",
    "                    # with _example_type removed from item[0] OrderedDict\n",
    "                    # all other keys should be part of input\n",
    "                    _input, label = item\n",
    "                    if adv != \"adversarial_example\":\n",
    "                        raise ValueError(\n",
    "                            \"`item` has length of 3 but last element is not for marking if the item is an `adversarial example`.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        is_adv_sample.append(True)\n",
    "                else:\n",
    "                    # else `len(item)` is 2.\n",
    "                    _input, label = item\n",
    "                    is_adv_sample.append(False)\n",
    "\n",
    "                if isinstance(_input, collections.OrderedDict):\n",
    "                    _input = tuple(_input.values())\n",
    "                else:\n",
    "                    _input = tuple(_input)\n",
    "\n",
    "                if len(_input) == 1:\n",
    "                    _input = _input[0]\n",
    "                input_texts.append(_input)\n",
    "                targets.append(label)\n",
    "\n",
    "            return input_texts, torch.tensor(targets), torch.tensor(is_adv_sample)\n",
    "\n",
    "        if adv_dataset:\n",
    "            dataset = torch.utils.data.ConcatDataset([dataset, adv_dataset])\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return train_dataloader\n",
    "\n",
    "    def get_eval_dataloader(self, dataset, batch_size):\n",
    "        \"\"\"Returns the :obj:`torch.utils.data.DataLoader` for evaluation.\n",
    "\n",
    "        Args:\n",
    "            dataset (:class:`~textattack.datasets.Dataset`):\n",
    "                Dataset to use for evaluation.\n",
    "            batch_size (:obj:`int`):\n",
    "                Batch size for evaluation.\n",
    "        Returns:\n",
    "            :obj:`torch.utils.data.DataLoader`\n",
    "        \"\"\"\n",
    "\n",
    "        # Helper functions for collating data\n",
    "        def collate_fn(data):\n",
    "            input_texts = []\n",
    "            targets = []\n",
    "            for _input, label in data:\n",
    "                if isinstance(_input, collections.OrderedDict):\n",
    "                    _input = tuple(_input.values())\n",
    "                else:\n",
    "                    _input = tuple(_input)\n",
    "\n",
    "                if len(_input) == 1:\n",
    "                    _input = _input[0]\n",
    "                input_texts.append(_input)\n",
    "                targets.append(label)\n",
    "            return input_texts, torch.tensor(targets)\n",
    "\n",
    "        eval_dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return eval_dataloader\n",
    "\n",
    "    def training_step(self, model, tokenizer, batch):\n",
    "        \"\"\"Perform a single training step on a batch of inputs.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`torch.nn.Module`):\n",
    "                Model to train.\n",
    "            tokenizer:\n",
    "                Tokenizer used to tokenize input text.\n",
    "            batch (:obj:`tuple[list[str], torch.Tensor, torch.Tensor]`):\n",
    "                By default, this will be a tuple of input texts, targets, and boolean tensor indicating if the sample is an adversarial example.\n",
    "\n",
    "                .. note::\n",
    "                    If you override the :meth:`get_train_dataloader` method, then shape/type of :obj:`batch` will depend on how you created your batch.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`tuple[torch.Tensor, torch.Tensor, torch.Tensor]` where\n",
    "\n",
    "            - **loss**: :obj:`torch.FloatTensor` of shape 1 containing the loss.\n",
    "            - **preds**: :obj:`torch.FloatTensor` of model's prediction for the batch.\n",
    "            - **targets**: :obj:`torch.Tensor` of model's targets (e.g. labels, target values).\n",
    "        \"\"\"\n",
    "\n",
    "        input_texts, targets, is_adv_sample = batch\n",
    "        _targets = targets\n",
    "        targets = targets.to(textattack.shared.utils.device)\n",
    "\n",
    "        if isinstance(model, transformers.PreTrainedModel) or (\n",
    "            isinstance(model, torch.nn.DataParallel)\n",
    "            and isinstance(model.module, transformers.PreTrainedModel)\n",
    "        ):\n",
    "            input_ids = tokenizer(\n",
    "                input_texts,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "            )\n",
    "            input_ids.to(textattack.shared.utils.device)\n",
    "            logits = model(**input_ids)[0]\n",
    "        else:\n",
    "            input_ids = tokenizer(input_texts)\n",
    "            if not isinstance(input_ids, torch.Tensor):\n",
    "                input_ids = torch.tensor(input_ids)\n",
    "            input_ids = input_ids.to(textattack.shared.utils.device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "        if self.task_type == \"regression\":\n",
    "            loss = self.loss_fct(logits.squeeze(), targets.squeeze())\n",
    "            preds = logits\n",
    "        else:\n",
    "            loss = self.loss_fct(logits, targets)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "        sample_weights = torch.ones(\n",
    "            is_adv_sample.size(), device=textattack.shared.utils.device\n",
    "        )\n",
    "        sample_weights[is_adv_sample] *= self.training_args.alpha\n",
    "        loss = loss * sample_weights\n",
    "        loss = torch.mean(loss)\n",
    "        preds = preds.cpu()\n",
    "\n",
    "        return loss, preds, _targets\n",
    "\n",
    "    def evaluate_step(self, model, tokenizer, batch):\n",
    "        \"\"\"Perform a single evaluation step on a batch of inputs.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`torch.nn.Module`):\n",
    "                Model to train.\n",
    "            tokenizer:\n",
    "                Tokenizer used to tokenize input text.\n",
    "            batch (:obj:`tuple[list[str], torch.Tensor]`):\n",
    "                By default, this will be a tuple of input texts and target tensors.\n",
    "\n",
    "                .. note::\n",
    "                    If you override the :meth:`get_eval_dataloader` method, then shape/type of :obj:`batch` will depend on how you created your batch.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`tuple[torch.Tensor, torch.Tensor]` where\n",
    "\n",
    "            - **preds**: :obj:`torch.FloatTensor` of model's prediction for the batch.\n",
    "            - **targets**: :obj:`torch.Tensor` of model's targets (e.g. labels, target values).\n",
    "        \"\"\"\n",
    "        input_texts, targets = batch\n",
    "        _targets = targets\n",
    "        targets = targets.to(textattack.shared.utils.device)\n",
    "\n",
    "        if isinstance(model, transformers.PreTrainedModel):\n",
    "            input_ids = tokenizer(\n",
    "                input_texts,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "            )\n",
    "            input_ids.to(textattack.shared.utils.device)\n",
    "            logits = model(**input_ids)[0]\n",
    "        else:\n",
    "            input_ids = tokenizer(input_texts)\n",
    "            if not isinstance(input_ids, torch.Tensor):\n",
    "                input_ids = torch.tensor(input_ids)\n",
    "            input_ids = input_ids.to(textattack.shared.utils.device)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "        if self.task_type == \"regression\":\n",
    "            preds = logits\n",
    "        else:\n",
    "            preds = logits.argmax(dim=-1)\n",
    "\n",
    "        return preds.cpu(), _targets\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the model on given training dataset.\"\"\"\n",
    "        if not self.train_dataset:\n",
    "            raise ValueError(\"No `train_dataset` available for training.\")\n",
    "\n",
    "        textattack.shared.utils.set_seed(self.training_args.random_seed)\n",
    "        if not os.path.exists(self.training_args.output_dir):\n",
    "            os.makedirs(self.training_args.output_dir)\n",
    "\n",
    "        # Save logger writes to file\n",
    "        log_txt_path = os.path.join(self.training_args.output_dir, \"train_log.txt\")\n",
    "        fh = logging.FileHandler(log_txt_path)\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        logger.addHandler(fh)\n",
    "        logger.info(f\"Writing logs to {log_txt_path}.\")\n",
    "\n",
    "        # Save original self.training_args to file\n",
    "        args_save_path = os.path.join(\n",
    "            self.training_args.output_dir, \"training_args.json\"\n",
    "        )\n",
    "        with open(args_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.training_args.__dict__, f)\n",
    "        logger.info(f\"Wrote original training args to {args_save_path}.\")\n",
    "\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        tokenizer = self.model_wrapper.tokenizer\n",
    "        model = self.model_wrapper.model\n",
    "\n",
    "        if self.training_args.parallel and num_gpus > 1:\n",
    "            # TODO: torch.nn.parallel.DistributedDataParallel\n",
    "            # Supposedly faster than DataParallel, but requires more work to setup properly.\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            logger.info(f\"Training on {num_gpus} GPUs via `torch.nn.DataParallel`.\")\n",
    "            train_batch_size = self.training_args.per_device_train_batch_size * num_gpus\n",
    "        else:\n",
    "            train_batch_size = self.training_args.per_device_train_batch_size\n",
    "\n",
    "        if self.attack is None:\n",
    "            num_clean_epochs = self.training_args.num_epochs\n",
    "        else:\n",
    "            num_clean_epochs = self.training_args.num_clean_epochs\n",
    "\n",
    "        total_clean_training_steps = (\n",
    "            math.ceil(\n",
    "                len(self.train_dataset)\n",
    "                / (train_batch_size * self.training_args.gradient_accumulation_steps)\n",
    "            )\n",
    "            * num_clean_epochs\n",
    "        )\n",
    "\n",
    "        # calculate total_adv_training_data_length based on type of\n",
    "        # num_train_adv_examples.\n",
    "        # if num_train_adv_examples is float , num_train_adv_examples is a portion of train_dataset.\n",
    "        if isinstance(self.training_args.num_train_adv_examples, float):\n",
    "            total_adv_training_data_length = (\n",
    "                len(self.train_dataset) * self.training_args.num_train_adv_examples\n",
    "            )\n",
    "\n",
    "        # if num_train_adv_examples is int and >=0 then it is taken as value.\n",
    "        elif (\n",
    "            isinstance(self.training_args.num_train_adv_examples, int)\n",
    "            and self.training_args.num_train_adv_examples >= 0\n",
    "        ):\n",
    "            total_adv_training_data_length = self.training_args.num_train_adv_examples\n",
    "\n",
    "        # if num_train_adv_examples is = -1 , we generate all possible adv examples.\n",
    "        # Max number of all possible adv examples would be equal to train_dataset.\n",
    "        else:\n",
    "            total_adv_training_data_length = len(self.train_dataset)\n",
    "\n",
    "        # Based on total_adv_training_data_length calculation , find total total_adv_training_steps\n",
    "        total_adv_training_steps = math.ceil(\n",
    "            (len(self.train_dataset) + total_adv_training_data_length)\n",
    "            / (train_batch_size * self.training_args.gradient_accumulation_steps)\n",
    "        ) * (self.training_args.num_epochs - num_clean_epochs)\n",
    "\n",
    "        total_training_steps = total_clean_training_steps + total_adv_training_steps\n",
    "\n",
    "        optimizer, scheduler = self.get_optimizer_and_scheduler(\n",
    "            model, total_training_steps\n",
    "        )\n",
    "\n",
    "        self._print_training_args(\n",
    "            total_training_steps, train_batch_size, num_clean_epochs\n",
    "        )\n",
    "\n",
    "        model.to(textattack.shared.utils.device)\n",
    "\n",
    "        # Variables across epochs\n",
    "        self._total_loss = 0.0\n",
    "        self._current_loss = 0.0\n",
    "        self._last_log_step = 0\n",
    "\n",
    "        # `best_score` is used to keep track of the best model across training.\n",
    "        # Could be loss, accuracy, or other metrics.\n",
    "        best_eval_score = 0.0\n",
    "        best_eval_score_epoch = 0\n",
    "        best_model_path = None\n",
    "        epochs_since_best_eval_score = 0\n",
    "\n",
    "        for epoch in range(1, self.training_args.num_epochs + 1):\n",
    "            logger.info(\"==========================================================\")\n",
    "            logger.info(f\"Epoch {epoch}\")\n",
    "\n",
    "            if self.attack and epoch > num_clean_epochs:\n",
    "                if (\n",
    "                    epoch - num_clean_epochs - 1\n",
    "                ) % self.training_args.attack_epoch_interval == 0:\n",
    "                    # only generate a new adversarial training set every self.training_args.attack_period epochs after the clean epochs\n",
    "                    # adv_dataset is instance of `textattack.datasets.Dataset`\n",
    "                    model.eval()\n",
    "                    adv_dataset = self._generate_adversarial_examples(epoch)\n",
    "                    model.train()\n",
    "                    model.to(textattack.shared.utils.device)\n",
    "                else:\n",
    "                    adv_dataset = None\n",
    "            else:\n",
    "                logger.info(f\"Running clean epoch {epoch}/{num_clean_epochs}\")\n",
    "                adv_dataset = None\n",
    "\n",
    "            train_dataloader = self.get_train_dataloader(\n",
    "                self.train_dataset, adv_dataset, train_batch_size\n",
    "            )\n",
    "            model.train()\n",
    "            # Epoch variables\n",
    "            all_preds = []\n",
    "            all_targets = []\n",
    "            prog_bar = tqdm.tqdm(\n",
    "                train_dataloader,\n",
    "                desc=\"Iteration\",\n",
    "                position=0,\n",
    "                leave=True,\n",
    "                dynamic_ncols=True,\n",
    "            )\n",
    "            for step, batch in enumerate(prog_bar):\n",
    "                loss, preds, targets = self.training_step(model, tokenizer, batch)\n",
    "\n",
    "                if isinstance(model, torch.nn.DataParallel):\n",
    "                    loss = loss.mean()\n",
    "\n",
    "                loss = loss / self.training_args.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "                loss = loss.item()\n",
    "                self._total_loss += loss\n",
    "                self._current_loss += loss\n",
    "\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(targets)\n",
    "\n",
    "                if (step + 1) % self.training_args.gradient_accumulation_steps == 0:\n",
    "                    optimizer.step()\n",
    "                    if scheduler:\n",
    "                        scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    self._global_step += 1\n",
    "\n",
    "                if self._global_step > 0:\n",
    "                    prog_bar.set_description(\n",
    "                        f\"Loss {self._total_loss/self._global_step:.5f}\"\n",
    "                    )\n",
    "\n",
    "                # TODO: Better way to handle TB and Wandb logging\n",
    "                if (self._global_step > 0) and (\n",
    "                    self._global_step % self.training_args.logging_interval_step == 0\n",
    "                ):\n",
    "                    lr_to_log = (\n",
    "                        scheduler.get_last_lr()[0]\n",
    "                        if scheduler\n",
    "                        else self.training_args.learning_rate\n",
    "                    )\n",
    "                    if self._global_step - self._last_log_step >= 1:\n",
    "                        loss_to_log = round(\n",
    "                            self._current_loss\n",
    "                            / (self._global_step - self._last_log_step),\n",
    "                            4,\n",
    "                        )\n",
    "                    else:\n",
    "                        loss_to_log = round(self._current_loss, 4)\n",
    "\n",
    "                    log = {\"train/loss\": loss_to_log, \"train/learning_rate\": lr_to_log}\n",
    "                    if self.training_args.log_to_tb:\n",
    "                        self._tb_log(log, self._global_step)\n",
    "\n",
    "                    if self.training_args.log_to_wandb:\n",
    "                        self._wandb_log(log, self._global_step)\n",
    "\n",
    "                    self._current_loss = 0.0\n",
    "                    self._last_log_step = self._global_step\n",
    "\n",
    "                # Save model checkpoint to file.\n",
    "                if self.training_args.checkpoint_interval_steps:\n",
    "                    if (\n",
    "                        self._global_step > 0\n",
    "                        and (\n",
    "                            self._global_step\n",
    "                            % self.training_args.checkpoint_interval_steps\n",
    "                        )\n",
    "                        == 0\n",
    "                    ):\n",
    "                        self._save_model_checkpoint(\n",
    "                            model, tokenizer, step=self._global_step\n",
    "                        )\n",
    "\n",
    "            preds = torch.cat(all_preds)\n",
    "            targets = torch.cat(all_targets)\n",
    "            if self._metric_name == \"accuracy\":\n",
    "                correct_predictions = (preds == targets).sum().item()\n",
    "                accuracy = correct_predictions / len(targets)\n",
    "                metric_log = {\"train/train_accuracy\": accuracy}\n",
    "                logger.info(f\"Train accuracy: {accuracy*100:.2f}%\")\n",
    "            else:\n",
    "                pearson_correlation, pearson_pvalue = scipy.stats.pearsonr(\n",
    "                    preds, targets\n",
    "                )\n",
    "                metric_log = {\n",
    "                    \"train/pearson_correlation\": pearson_correlation,\n",
    "                    \"train/pearson_pvalue\": pearson_pvalue,\n",
    "                }\n",
    "                logger.info(f\"Train Pearson correlation: {pearson_correlation:.4f}%\")\n",
    "\n",
    "            if len(targets) > 0:\n",
    "                if self.training_args.log_to_tb:\n",
    "                    self._tb_log(metric_log, epoch)\n",
    "                if self.training_args.log_to_wandb:\n",
    "                    metric_log[\"epoch\"] = epoch\n",
    "                    self._wandb_log(metric_log, self._global_step)\n",
    "\n",
    "            # Evaluate after each epoch.\n",
    "            eval_score = self.evaluate()\n",
    "\n",
    "            if self.training_args.log_to_tb:\n",
    "                self._tb_log({f\"eval/{self._metric_name}\": eval_score}, epoch)\n",
    "            if self.training_args.log_to_wandb:\n",
    "                self._wandb_log(\n",
    "                    {f\"eval/{self._metric_name}\": eval_score, \"epoch\": epoch},\n",
    "                    self._global_step,\n",
    "                )\n",
    "\n",
    "            if (\n",
    "                self.training_args.checkpoint_interval_epochs\n",
    "                and (epoch % self.training_args.checkpoint_interval_epochs) == 0\n",
    "            ):\n",
    "                self._save_model_checkpoint(model, tokenizer, epoch=epoch)\n",
    "\n",
    "            if eval_score > best_eval_score:\n",
    "                best_eval_score = eval_score\n",
    "                best_eval_score_epoch = epoch\n",
    "                epochs_since_best_eval_score = 0\n",
    "                self._save_model_checkpoint(model, tokenizer, best=True)\n",
    "                logger.info(\n",
    "                    f\"Best score found. Saved model to {self.training_args.output_dir}/best_model/\"\n",
    "                )\n",
    "            else:\n",
    "                epochs_since_best_eval_score += 1\n",
    "                if self.training_args.early_stopping_epochs and (\n",
    "                    epochs_since_best_eval_score\n",
    "                    > self.training_args.early_stopping_epochs\n",
    "                ):\n",
    "                    logger.info(\n",
    "                        f\"Stopping early since it's been {self.training_args.early_stopping_epochs} steps since validation score increased.\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "        if self.training_args.log_to_tb:\n",
    "            self._tb_writer.flush()\n",
    "\n",
    "        # Finish training\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            model = model.module\n",
    "\n",
    "        if self.training_args.load_best_model_at_end:\n",
    "            best_model_path = os.path.join(self.training_args.output_dir, \"best_model\")\n",
    "            if hasattr(model, \"from_pretrained\"):\n",
    "                model = model.__class__.from_pretrained(best_model_path)\n",
    "            else:\n",
    "                model = model.load_state_dict(\n",
    "                    torch.load(os.path.join(best_model_path, \"pytorch_model.bin\"))\n",
    "                )\n",
    "\n",
    "        if self.training_args.save_last:\n",
    "            self._save_model_checkpoint(model, tokenizer, last=True)\n",
    "\n",
    "        self.model_wrapper.model = model\n",
    "        self._write_readme(best_eval_score, best_eval_score_epoch, train_batch_size)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model on given evaluation dataset.\"\"\"\n",
    "\n",
    "        if not self.eval_dataset:\n",
    "            raise ValueError(\"No `eval_dataset` available for training.\")\n",
    "\n",
    "        logging.info(\"Evaluating model on evaluation dataset.\")\n",
    "        model = self.model_wrapper.model\n",
    "        tokenizer = self.model_wrapper.tokenizer\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            num_gpus = torch.cuda.device_count()\n",
    "            eval_batch_size = self.training_args.per_device_eval_batch_size * num_gpus\n",
    "        else:\n",
    "            eval_batch_size = self.training_args.per_device_eval_batch_size\n",
    "\n",
    "        eval_dataloader = self.get_eval_dataloader(self.eval_dataset, eval_batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(eval_dataloader):\n",
    "                preds, targets = self.evaluate_step(model, tokenizer, batch)\n",
    "                all_preds.append(preds)\n",
    "                all_targets.append(targets)\n",
    "\n",
    "        preds = torch.cat(all_preds)\n",
    "        targets = torch.cat(all_targets)\n",
    "\n",
    "        if self.task_type == \"regression\":\n",
    "            pearson_correlation, pearson_p_value = scipy.stats.pearsonr(preds, targets)\n",
    "            eval_score = pearson_correlation\n",
    "        else:\n",
    "            correct_predictions = (preds == targets).sum().item()\n",
    "            accuracy = correct_predictions / len(targets)\n",
    "            eval_score = accuracy\n",
    "\n",
    "        if self._metric_name == \"accuracy\":\n",
    "            logger.info(f\"Eval {self._metric_name}: {eval_score*100:.2f}%\")\n",
    "        else:\n",
    "            logger.info(f\"Eval {self._metric_name}: {eval_score:.4f}%\")\n",
    "\n",
    "        return eval_score\n",
    "\n",
    "    def _write_readme(self, best_eval_score, best_eval_score_epoch, train_batch_size):\n",
    "        if isinstance(self.training_args, CommandLineTrainingArgs):\n",
    "            model_name = self.training_args.model_name_or_path\n",
    "        elif isinstance(self.model_wrapper.model, transformers.PreTrainedModel):\n",
    "            if (\n",
    "                hasattr(self.model_wrapper.model.config, \"_name_or_path\")\n",
    "                and self.model_wrapper.model.config._name_or_path in HUGGINGFACE_MODELS\n",
    "            ):\n",
    "                # TODO Better way than just checking HUGGINGFACE_MODELS ?\n",
    "                model_name = self.model_wrapper.model.config._name_or_path\n",
    "            elif hasattr(self.model_wrapper.model.config, \"model_type\"):\n",
    "                model_name = self.model_wrapper.model.config.model_type\n",
    "            else:\n",
    "                model_name = \"\"\n",
    "        else:\n",
    "            model_name = \"\"\n",
    "\n",
    "        if model_name:\n",
    "            model_name = f\"`{model_name}`\"\n",
    "\n",
    "        if (\n",
    "            isinstance(self.training_args, CommandLineTrainingArgs)\n",
    "            and self.training_args.model_max_length\n",
    "        ):\n",
    "            model_max_length = self.training_args.model_max_length\n",
    "        elif isinstance(\n",
    "            self.model_wrapper.model,\n",
    "            (\n",
    "                transformers.PreTrainedModel,\n",
    "                LSTMForClassification,\n",
    "                WordCNNForClassification,\n",
    "            ),\n",
    "        ):\n",
    "            model_max_length = self.model_wrapper.tokenizer.model_max_length\n",
    "        else:\n",
    "            model_max_length = None\n",
    "\n",
    "        if model_max_length:\n",
    "            model_max_length_str = f\" a maximum sequence length of {model_max_length},\"\n",
    "        else:\n",
    "            model_max_length_str = \"\"\n",
    "\n",
    "        if isinstance(\n",
    "            self.train_dataset, textattack.datasets.HuggingFaceDataset\n",
    "        ) and hasattr(self.train_dataset, \"_name\"):\n",
    "            dataset_name = self.train_dataset._name\n",
    "            if hasattr(self.train_dataset, \"_subset\"):\n",
    "                dataset_name += f\" ({self.train_dataset._subset})\"\n",
    "        elif isinstance(\n",
    "            self.eval_dataset, textattack.datasets.HuggingFaceDataset\n",
    "        ) and hasattr(self.eval_dataset, \"_name\"):\n",
    "            dataset_name = self.eval_dataset._name\n",
    "            if hasattr(self.eval_dataset, \"_subset\"):\n",
    "                dataset_name += f\" ({self.eval_dataset._subset})\"\n",
    "        else:\n",
    "            dataset_name = None\n",
    "\n",
    "        if dataset_name:\n",
    "            dataset_str = (\n",
    "                \"and the `{dataset_name}` dataset loaded using the `datasets` library\"\n",
    "            )\n",
    "        else:\n",
    "            dataset_str = \"\"\n",
    "\n",
    "        loss_func = (\n",
    "            \"mean squared error\" if self.task_type == \"regression\" else \"cross-entropy\"\n",
    "        )\n",
    "        metric_name = (\n",
    "            \"pearson correlation\" if self.task_type == \"regression\" else \"accuracy\"\n",
    "        )\n",
    "        epoch_info = f\"{best_eval_score_epoch} epoch\" + (\n",
    "            \"s\" if best_eval_score_epoch > 1 else \"\"\n",
    "        )\n",
    "        readme_text = f\"\"\"\n",
    "            ## TextAttack Model Card\n",
    "\n",
    "            This {model_name} model was fine-tuned using TextAttack{dataset_str}. The model was fine-tuned\n",
    "            for {self.training_args.num_epochs} epochs with a batch size of {train_batch_size},\n",
    "            {model_max_length_str} and an initial learning rate of {self.training_args.learning_rate}.\n",
    "            Since this was a {self.task_type} task, the model was trained with a {loss_func} loss function.\n",
    "            The best score the model achieved on this task was {best_eval_score}, as measured by the\n",
    "            eval set {metric_name}, found after {epoch_info}.\n",
    "\n",
    "            For more information, check out [TextAttack on Github](https://github.com/QData/TextAttack).\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "        readme_save_path = os.path.join(self.training_args.output_dir, \"README.md\")\n",
    "        with open(readme_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(readme_text.strip() + \"\\n\")\n",
    "        logger.info(f\"Wrote README to {readme_save_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nis3353",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
